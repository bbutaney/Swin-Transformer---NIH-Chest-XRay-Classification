{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cells were run in Google Colab to train a Swin Transformer for image classification on the NIH dataset of ~100k chest x-rays.\n",
        "\n",
        "This is a multiclass, multilabel problem, with images being classified as showing one or more of 15 diseases.\n",
        "\n",
        "The files used in this notebook can be downloaded here: https://www.kaggle.com/datasets/nih-chest-xrays/data. A description of the data can be\n",
        "found here: https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community.\n",
        "\n",
        "Achieved accuracy of ~80% trained from scratch. SoTA is ~82%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow8KXJwmByFj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "path = '/content/drive/MyDrive/buffer/testing_swin'\n",
        "\n",
        "# put train and test files from txt to dict\n",
        "loc_dict = {} # call loc_dict[filename] to see if image is test\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cxr_indiv/test_list.txt\", \"r\") as a_file:\n",
        "  for line in a_file:\n",
        "    # IF IN THE CURRENT IMAGE FOLDER, THEN\n",
        "    stripped_line = line.strip()\n",
        "    loc_dict[stripped_line] = a_file.name.replace(\"/content/drive/MyDrive/cxr_indiv/\", \"\").replace(\"_list.txt\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysp97xrTLUvz",
        "outputId": "5262bf8a-b533-4324-e771-68130c6dd688"
      },
      "outputs": [],
      "source": [
        "# split csv into train and test\n",
        "# loop through csv file\n",
        "# with conditional to separate train and test images\n",
        "\n",
        "import pandas as pd\n",
        "full_csv = pd.read_csv(\"/content/drive/MyDrive/cxr_indiv/Data_Entry_2017_v2020.csv\")\n",
        "\n",
        "test_df = pd.DataFrame({})\n",
        "for i in range(len(full_csv['Image Index'])):\n",
        "  try:\n",
        "    loc_dict[full_csv['Image Index'][i]] == 'test'\n",
        "    if loc_dict[full_csv['Image Index'][i]] == 'test':\n",
        "      test_df = test_df.append(full_csv.iloc[i])\n",
        "    else:\n",
        "      print(\"error\")\n",
        "  except KeyError:\n",
        "    continue\n",
        "\n",
        "# create sets of labels and remove no finding\n",
        "# =========================== For Testing ===========================\n",
        "\n",
        "for i in range(len(test_df['Finding Labels'])):\n",
        "  if type(test_df['Finding Labels'].iloc[i]) != set:\n",
        "    temp = test_df['Finding Labels'].iloc[i].split('|')\n",
        "    if 'No Finding' in temp:\n",
        "      temp.remove('No Finding')\n",
        "    test_df['Finding Labels'].iloc[i] = set(temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsUFB8oGMeBp",
        "outputId": "778ed6a9-1ac8-4d78-8596-9e48da1e5f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.6.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "# !pip install torch\n",
        "# !pip install torchvision\n",
        "# !pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMHRcfenNv_N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import torch\n",
        "import io\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "import torchvision\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        temp = annotations_file['Finding Labels']\n",
        "        mlb = sklearn.preprocessing.MultiLabelBinarizer()\n",
        "        self.img_labels = pd.DataFrame(mlb.fit_transform(temp),columns=mlb.classes_)\n",
        "        print(mlb.classes_)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.annot = annotations_file\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.annot['Image Index'].iloc[idx])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        image.resize_(3, 224, 224)\n",
        "        return image, label.to_numpy() # torch tensor, numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pxMLHGJOCRM",
        "outputId": "97bcabb3-af14-48a8-cc89-21fa1c08ef63"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "import torchvision\n",
        "test_dataset = CustomImageDataset(test_df, path)\n",
        "\n",
        "# create a data loader for train, valid, and test sets\n",
        "batches = 100\n",
        "\n",
        "test_dl = DataLoader(test_dataset, batch_size=batches, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4FNocA-ORcx",
        "outputId": "5d229618-ae78-4d3f-f1f7-5dfd3a23d20c"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPZIyjocOVvV",
        "outputId": "d055d8ec-631d-40a0-eaa0-33f876d7046f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "import collections\n",
        "try:\n",
        "    from collections import OrderedDict\n",
        "except ImportError:\n",
        "    OrderedDict = dict\n",
        "\n",
        "HUB_URL = \"SharanSMenon/swin-transformer-hub:main\"\n",
        "MODEL_NAME = \"swin_tiny_patch4_window7_224\"\n",
        "model = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=False)\n",
        "classifier = nn.Sequential(nn.Linear(model.head.in_features, 14), nn.Sigmoid())\n",
        "model.classifier = classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10rHhCL7OYFf",
        "outputId": "c671f8b9-6d05-4e9b-9e45-372e3202a6b3"
      },
      "outputs": [],
      "source": [
        "modelpath = 'state_12.pth'\n",
        "modelpath = '/content/drive/MyDrive/tesim/'+ modelpath\n",
        "state = torch.load(modelpath)\n",
        "model.load_state_dict(state['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF3f13osP52Z"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXrDlIwiQCPD",
        "outputId": "71e9cb8e-c13f-4619-bd8a-f66376128c14"
      },
      "outputs": [],
      "source": [
        "from torch.autograd.grad_mode import F\n",
        "from sklearn.metrics import confusion_matrix\n",
        "pred = torch.Tensor([[0,1, 1, 0], [1,1,1,1]])\n",
        "truth = torch.Tensor([[1, 1, 0, 0], [1,1,1,1]])\n",
        "\n",
        "def find_single_accuracy(pred, truth):\n",
        "  length = 0\n",
        "  true_neg_and_pos = 0\n",
        "\n",
        "  # get tensor size\n",
        "  for i in pred:\n",
        "    length+=1\n",
        "\n",
        "  # count true positives and negatives\n",
        "  for i in range(length):\n",
        "    if pred[i].item() == truth[i].item():\n",
        "      true_neg_and_pos+=1\n",
        "\n",
        "  return true_neg_and_pos / length\n",
        "\n",
        "\n",
        "\n",
        "def find_accuracy(pred: torch.Tensor, truth: torch.Tensor):\n",
        "  \"\"\" returns accuracy given a k-D prediction (one hot encoded) and the target for those k samples\n",
        "  \"\"\"\n",
        "  accuracies = []\n",
        "  length = 0\n",
        "  for i in pred:\n",
        "    length+=1\n",
        "\n",
        "  for i in range(length):\n",
        "    accuracies.append(find_single_accuracy(pred[i], truth[i]))\n",
        "\n",
        "  return sum(accuracies) / len(accuracies)\n",
        "\n",
        "print(find_accuracy(pred, truth))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJFI-GD94bxc"
      },
      "source": [
        "ROC_AUC Score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rid1bZEe4d5j",
        "outputId": "ebe7f6b1-abe5-428c-f4de-ab25dd9fd6dd"
      },
      "outputs": [],
      "source": [
        "from numpy import vstack\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Softmax\n",
        "from torch.nn import Module\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "running_loss = 0\n",
        "current_batch_accs = []\n",
        "epoch_accs = []\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# testing loop\n",
        "model.eval()\n",
        "predictions, actuals = list(), list()\n",
        "torch.cuda.empty_cache()\n",
        "min_valid_loss = float('inf')\n",
        "valid_loss = 0\n",
        "for (inputs, targets) in tqdm(test_dl):\n",
        "  inputs = inputs.to(device)\n",
        "  targets = targets.to(device)\n",
        "  # compute the model output and establish layers\n",
        "  lin = nn.Linear(1000, 14)\n",
        "  lin = lin.to(device)\n",
        "  sig = nn.Sigmoid()\n",
        "  yhat = model(inputs.float())\n",
        "  yhat = lin(yhat)\n",
        "  yhat = sig(yhat)\n",
        "  yhat = torch.round(yhat, decimals=0) # Perhaps remove for roc_auc\n",
        "  yhat = yhat.detach()\n",
        "\n",
        "  actual = targets.cpu().float().numpy()\n",
        "\n",
        "  predictions.append(yhat)\n",
        "  actuals.append(actual)\n",
        "  yhat = yhat.detach()\n",
        "  torch.cuda.empty_cache()\n",
        "  # calculate batch accuracy\n",
        "  # print(yhat)\n",
        "  print('\\n')\n",
        "  print(actual)\n",
        "  try:\n",
        "    print('batch roc_auc_score:', str(roc_auc_score(y_true=actual, y_score=yhat.cpu())))\n",
        "    current_batch_accs.append(roc_auc_score(y_true=actual, y_score=yhat))\n",
        "  except:\n",
        "    print('error')\n",
        "    actual = np.vstack((actual, np.array([1,0,0,0,0,0,0,0,0,0,0,0,0,0])))\n",
        "    yhat = np.vstack((yhat.cpu(), np.array([1,0,0,0,0,0,0,0,0,0,0,0,0,0])))\n",
        "    print('batch roc_auc_score:', str(roc_auc_score(y_true=actual.flatten(), y_score=yhat.flatten())))\n",
        "    current_batch_accs.append(roc_auc_score(y_true=actual.flatten(), y_score=yhat.flatten()))\n",
        "  print('to-date average roc_auc_score', str(sum(current_batch_accs) / len(current_batch_accs)))\n",
        "\n",
        "# calculate epoch accuracy\n",
        "print('total test accuracy: ' + str(sum(current_batch_accs) / len(current_batch_accs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFGE_Dlj4Y7T"
      },
      "source": [
        "Confusion Matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6wfQFn8QGEM"
      },
      "outputs": [],
      "source": [
        "from numpy import vstack\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Softmax\n",
        "from torch.nn import Module\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "running_loss = 0\n",
        "current_batch_accs = []\n",
        "epoch_accs = []\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# testing loop\n",
        "model.eval()\n",
        "predictions, actuals = list(), list()\n",
        "torch.cuda.empty_cache()\n",
        "min_valid_loss = float('inf')\n",
        "valid_loss = 0\n",
        "for (inputs, targets) in tqdm(test_dl):\n",
        "  inputs = inputs.to(device)\n",
        "  targets = targets.to(device)\n",
        "  # compute the model output and establish layers\n",
        "  lin = nn.Linear(1000, 14)\n",
        "  lin = lin.to(device)\n",
        "  sig = nn.Sigmoid()\n",
        "  yhat = model(inputs.float())\n",
        "  yhat = lin(yhat)\n",
        "  yhat = sig(yhat)\n",
        "  yhat = torch.round(yhat, decimals=0)\n",
        "  yhat = yhat.detach()\n",
        "\n",
        "  actual = targets.cpu().float().numpy()\n",
        "\n",
        "  predictions.append(yhat)\n",
        "  actuals.append(actual)\n",
        "  yhat = yhat.detach()\n",
        "  torch.cuda.empty_cache()\n",
        "  # calculate batch accuracy\n",
        "  # print(yhat)\n",
        "  print('\\n')\n",
        "  print('batch accuracy:', str(find_accuracy(yhat, actual)))\n",
        "  current_batch_accs.append(find_accuracy(yhat, actual))\n",
        "  print('to-date average accuracy', str(sum(current_batch_accs) / len(current_batch_accs)))\n",
        "\n",
        "# calculate epoch accuracy\n",
        "predictions, actuals = vstack([i.cpu() for i in predictions]), vstack(actuals)\n",
        "acc = (actuals, predictions)\n",
        "print('total test accuracy: ' + str(sum(current_batch_accs) / len(current_batch_accs)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Swin_Transformer_Testing_Loop",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
